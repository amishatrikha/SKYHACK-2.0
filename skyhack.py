# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13t5k73ngBimevbBNkWkNn4RqKkvahRGC
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler

# Step 1: Load the CSV files
test_df = pd.read_csv('TEST.csv')
reason_df = pd.read_csv('REASON.csv')
calls_df = pd.read_csv('CALLS.csv')
customers_df = pd.read_csv('CUSTOMERS.csv')
sentiment_df = pd.read_csv('SENTIMENT.csv')

# Merge dataframes
merged_df = pd.merge(calls_df, reason_df, on='call_id', how='left')
merged_df = pd.merge(merged_df, customers_df, on='customer_id', how='left')
merged_df = pd.merge(merged_df, sentiment_df, on=['call_id', 'agent_id'], how='left')


# Convert datetime columns to pandas datetime objects
merged_df['call_start_datetime'] = pd.to_datetime(merged_df['call_start_datetime'])
merged_df['call_end_datetime'] = pd.to_datetime(merged_df['call_end_datetime'])
merged_df['agent_assigned_datetime'] = pd.to_datetime(merged_df['agent_assigned_datetime'])

# Calculate AHT (Average Handle Time)
merged_df['AHT'] = (merged_df['call_end_datetime'] - merged_df['call_start_datetime']).dt.total_seconds()

# Calculate AST (Agent Service Time)
merged_df['AST'] = (merged_df['call_end_datetime'] - merged_df['agent_assigned_datetime']).dt.total_seconds()

# Handle Missing Data
# Check for missing values
print("Missing data counts before filling:\n", merged_df.isnull().sum())

# Fill missing values in numeric columns with the median
numeric_cols = merged_df.select_dtypes(include=['float64', 'int64']).columns
merged_df[numeric_cols] = merged_df[numeric_cols].fillna(merged_df[numeric_cols].median())

# Fill missing values in categorical columns with the mode
categorical_cols = ['primary_call_reason', 'elite_level_code']
for col in categorical_cols:
    merged_df[col] = merged_df[col].fillna(merged_df[col].mode()[0])

# Fill missing values in text columns with empty strings
text_cols = ['call_transcript']
for col in text_cols:
    merged_df[col] = merged_df[col].fillna('')

# Verify that missing data has been handled
print("Missing data counts after filling:\n", merged_df.isnull().sum())

# Step 5: Save the processed data for further analysis or modeling
merged_df.to_csv('processed_data.csv', index=False)

# Step 6: Data Visualization
# Load the processed data
merged_df = pd.read_csv('processed_data.csv')

# Summary Statistics
print("Numerical Features Summary:\n", merged_df.describe())
for col in categorical_cols:
    print(f"\nValue counts for {col}:\n", merged_df[col].value_counts())

# Visualization of Key Metrics
plt.figure(figsize=(10, 6))
sns.histplot(merged_df['AHT'], bins=30, kde=True, color='blue')
plt.title('Distribution of Average Handle Time (AHT)')
plt.xlabel('AHT (seconds)')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(merged_df['AST'], bins=30, kde=True, color='green')
plt.title('Distribution of Agent Service Time (AST)')
plt.xlabel('AST (seconds)')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(data=merged_df, x='primary_call_reason', palette='Set2')
plt.title('Count of Primary Call Reasons')
plt.xticks(rotation=45, ha='right')
plt.xlabel('Primary Call Reason')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(data=merged_df, x='elite_level_code', y='average_sentiment', palette='coolwarm')
plt.title('Average Sentiment by Elite Level Code')
plt.xlabel('Elite Level Code')
plt.ylabel('Average Sentiment')
plt.show()

plt.figure(figsize=(10, 8))
corr_matrix = merged_df[['AHT', 'AST', 'average_sentiment', 'silence_percent_average']].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap of Key Features')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(data=merged_df, x='agent_tone', y='customer_tone', hue='average_sentiment', palette='coolwarm', size='average_sentiment')
plt.title('Agent Tone vs Customer Tone with Average Sentiment')
plt.xlabel('Agent Tone')
plt.ylabel('Customer Tone')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=merged_df, x='elite_level_code', palette='Set3')
plt.title('Customer Segmentation by Elite Level Code')
plt.xlabel('Elite Level Code')
plt.ylabel('Count')
plt.show()

# Step 7: Feature Engineering for Text Data
# Check if 'call_transcript' exists
if 'call_transcript' in merged_df.columns:
    tfidf_vectorizer = TfidfVectorizer(max_features=1000)
    call_transcript_tfidf = tfidf_vectorizer.fit_transform(merged_df['call_transcript'].fillna(''))
    tfidf_df = pd.DataFrame(call_transcript_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
    merged_df = pd.concat([merged_df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)
else:
    print("Warning: 'call_transcript' column not found in merged_df!")

# Step 8: One-Hot Encoding of Categorical Features
encoder = OneHotEncoder(sparse_output=False)
encoded_cols = pd.DataFrame(encoder.fit_transform(merged_df[['primary_call_reason', 'elite_level_code']]),
                             columns=encoder.get_feature_names_out(['primary_call_reason', 'elite_level_code']))
merged_df = pd.concat([merged_df.reset_index(drop=True), encoded_cols.reset_index(drop=True)], axis=1)
merged_df.drop(columns=['primary_call_reason', 'elite_level_code'], inplace=True)

# Display the final merged DataFrame structure
print("Final DataFrame structure:\n", merged_df.head())

# Step 9: Model Training with Classification
target_variable = 'average_sentiment'
bins = [0, 0.3, 0.6, 1.0]
labels = [0, 1, 2]
merged_df['sentiment_class'] = np.digitize(merged_df[target_variable], bins=bins)

# Drop non-numeric columns and keep only numeric features
X = merged_df.drop(columns=['sentiment_class', target_variable])
X_numeric = X.select_dtypes(include=['float64', 'int64'])
y = merged_df['sentiment_class']

# Split the Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X_numeric, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Random Forest Classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train_scaled, y_train)

# Make predictions and evaluate the model
y_pred = clf.predict(X_test_scaled)
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Accuracy Score:", accuracy_score(y_test, y_pred))